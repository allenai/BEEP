{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd8b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import csv\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import spacy\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.tokens import Span\n",
    "import medspacy\n",
    "from medspacy.context import ConTextComponent, ConTextRule\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f261adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ConText algorithm for negated entity detection\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tokenizer\",\"ner\"])\n",
    "context = ConTextComponent(nlp, rules=\"default\", use_context_window=True, max_scope=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d396d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add paths to files containing extracted mentions and raw texts here\n",
    "# In our pipeline, mentions are extracted using a model trained on the i2b2 2010 dataset\n",
    "mention_file = 'ADD PATH TO MENTION FILE'\n",
    "text_file = 'ADD PATH TO RAW TEXT FILE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "565c21a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in mentions and raw texts\n",
    "texts = {}\n",
    "reader = csv.reader(open(text_file))\n",
    "next(reader, None)\n",
    "for row in reader:\n",
    "    texts[row[0]] = []\n",
    "    sents = sent_tokenize(row[1])\n",
    "    for sent in sents:\n",
    "        texts[row[0]].append(list(word_tokenize(sent)))\n",
    "\n",
    "mentions = pickle.load(open(mention_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0017df35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processsing file 181614 (0)\n",
      "31 mentions kept out of 32 for file 181614\n",
      "Processsing file 136342 (1)\n",
      "17 mentions kept out of 18 for file 136342\n",
      "Processsing file 199961 (2)\n",
      "18 mentions kept out of 20 for file 199961\n",
      "Processsing file 121438 (3)\n",
      "63 mentions kept out of 63 for file 121438\n",
      "Processsing file 194393 (4)\n",
      "5 mentions kept out of 5 for file 194393\n",
      "Processsing file 138531 (5)\n",
      "68 mentions kept out of 73 for file 138531\n",
      "Processsing file 180762 (6)\n",
      "31 mentions kept out of 33 for file 180762\n",
      "Processsing file 166330 (7)\n",
      "34 mentions kept out of 43 for file 166330\n",
      "Processsing file 170119 (8)\n",
      "70 mentions kept out of 77 for file 170119\n",
      "Processsing file 194148 (9)\n"
     ]
    }
   ],
   "source": [
    "# Apply ConText algorithm to identify and filter negated entities\n",
    "filtered_mentions = {}\n",
    "file_ind = 0\n",
    "for file in texts:\n",
    "    print('Processsing file {} ({})'.format(file, file_ind))\n",
    "    file_ind += 1\n",
    "    filtered_mentions[file] = {}\n",
    "    for i, line in enumerate(texts[file]):\n",
    "        cur_ment = mentions[file][i]\n",
    "        if not cur_ment:\n",
    "            continue\n",
    "        # If mentions are present in sentence, perform negation-based filtering\n",
    "        filtered_mentions[file][i] = []\n",
    "        doc = Doc(nlp.vocab, words=line)\n",
    "        for name, proc in nlp.pipeline:\n",
    "            doc = proc(doc)\n",
    "        entities = []\n",
    "        for mention in cur_ment:\n",
    "            entities.append(Span(doc, mention['start_offset'], mention['end_offset']+1, mention['pred_type']))\n",
    "        doc.ents = tuple(entities)\n",
    "        doc = context(doc)\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            if ent._.is_negated:\n",
    "                continue\n",
    "            filtered_mentions[file][i].append({'mention': ent.text, 'start_offset': ent.start, 'end_offset': ent.end, 'pred_type': ent.label_})\n",
    "    all_mention_count = sum([len(y) for x,y in mentions[file].items()])\n",
    "    filtered_mention_count = sum([len(y) for x,y in filtered_mentions[file].items()])\n",
    "    print('{} mentions kept out of {} for file {}'.format(filtered_mention_count, all_mention_count, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e645650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MeSH entity linker to link filtered mentions\n",
    "from scispacy.linking import EntityLinker\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_md\",disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "nlp.add_pipe(\"merge_noun_chunks\")\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\"linker_name\": \"mesh\", \"resolve_abbreviations\": True})\n",
    "linker = nlp.get_pipe(\"scispacy_linker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7df52c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linking entities for file 181614 (0)\n",
      "Linking entities for file 136342 (1)\n",
      "Linking entities for file 199961 (2)\n",
      "Linking entities for file 121438 (3)\n",
      "Linking entities for file 194393 (4)\n",
      "Linking entities for file 138531 (5)\n",
      "Linking entities for file 180762 (6)\n",
      "Linking entities for file 166330 (7)\n",
      "Linking entities for file 170119 (8)\n"
     ]
    }
   ],
   "source": [
    "# Perform MeSH linking on filtered entities\n",
    "for i, file in enumerate(list(filtered_mentions.keys())):\n",
    "    print('Linking entities for file {} ({})'.format(file, i))\n",
    "    for sent in filtered_mentions[file]:\n",
    "        cur_ments = filtered_mentions[file][sent]\n",
    "        for mention in cur_ments:\n",
    "            doc = nlp(mention['mention'])\n",
    "            if not doc.ents:\n",
    "                continue\n",
    "            entities = doc.ents[0]\n",
    "            cuis = [cui for cui in entities._.kb_ents] #if cui[1]>=0.75]\n",
    "            if not cuis:\n",
    "                continue\n",
    "            mention['mesh_ids'] = cuis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "916b3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store final linked mentions (edit file name here)\n",
    "output_file = 'ADD PATH TO OUTPUT FILE'\n",
    "pickle.dump(filtered_mentions, open(output_file, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
