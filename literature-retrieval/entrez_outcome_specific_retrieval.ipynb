{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c457e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2dff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve articles from a specified database using a provided query string\n",
    "# Query string can be a single word/phrase or a list of words/phrase separated using '_'\n",
    "# Note that if a list of words/phrases is provided, this search will require every term\n",
    "# to be present in any articles it retrieves (i.e., 'AND' operation for multiple-term lists)\n",
    "# TODO: Please add your tool name and email ID in the base_url variable\n",
    "\n",
    "def db_extract(db, query):\n",
    "\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db={}&tool=TOOLNAME&email=EMAILID&retmax=100000&term=\".format(db)\n",
    "\n",
    "    article_ids = set()\n",
    "    query = query.split('_')\n",
    "    query = '+'.join([\"%22\"+x.replace(\" \", \"%20\")+\"%22[MeSH Terms]\" for x in query])\n",
    "    print(\"Running query: {}\".format(query))\n",
    "    query_url = base_url + query\n",
    "\n",
    "    response = requests.get(query_url)\n",
    "    root = ET.fromstring(response.content)\n",
    "    count = root.find(\"Count\").text\n",
    "    id_list = root.find(\"IdList\").findall(\"Id\")\n",
    "    article_ids.update([x.text for x in id_list])\n",
    "    print(len(article_ids))\n",
    "    if int(count) > 100000:\n",
    "        cur = 100000\n",
    "        while cur < int(count):\n",
    "            new_query = base_url + query + \"&retstart={}\".format(cur)\n",
    "            print(\"Running additional query: {}\".format(query))\n",
    "            response = requests.get(new_query)\n",
    "            root = ET.fromstring(response.content)\n",
    "            id_list = root.find(\"IdList\").findall(\"Id\")\n",
    "            cur += len(id_list)\n",
    "            article_ids.update([x.text for x in id_list])\n",
    "            print(len(article_ids))\n",
    "        print('Retrieved {}/{} results'.format(cur, count))\n",
    "    else:\n",
    "        print('Retrieved {} results'.format(count))\n",
    "    return article_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2699f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query: %22hospital%20mortality%22[MeSH Terms]\n",
      "9614\n",
      "Retrieved 9614 results\n",
      "Running query: %22mortality%22[MeSH Terms]+%22risk%20factors%22[MeSH Terms]+%22humans%22[MeSH Terms]\n",
      "12408\n",
      "Retrieved 12408 results\n",
      "Running query: %22hospital%20mortality%22[MeSH Terms]\n",
      "47756\n",
      "Retrieved 47756 results\n",
      "Running query: %22mortality%22[MeSH Terms]+%22risk%20factors%22[MeSH Terms]+%22humans%22[MeSH Terms]\n",
      "62261\n",
      "Retrieved 62261 results\n"
     ]
    }
   ],
   "source": [
    "# Example querying procedure for mortality outcome\n",
    "# TODO: Specify query terms according to your outcomes of interest\n",
    "\n",
    "# Retrieve mortality related articles from the PMC database\n",
    "pmc_ids = db_extract(\"pmc\", \"hospital mortality\")\n",
    "pmc_ids = pmc_ids.union(db_extract(\"pmc\", \"mortality_risk factors_humans\"))\n",
    "\n",
    "# Retrieve mortality related articles from the PubMed database\n",
    "pubmed_ids = db_extract(\"pubmed\", \"hospital mortality\")\n",
    "pubmed_ids = pubmed_ids.union(db_extract(\"pubmed\", \"mortality_risk factors_humans\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c6d510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure to combine articles retrieved from both PMC and PubMed databases\n",
    "# To do this combination, PMC article IDs need to be mapped to their corresponding PubMed IDs first\n",
    "# to avoid double-counting of articles included in both databases\n",
    "def combine_ids(pmc, pubmed):\n",
    "    reader = csv.reader(open('../data/PMC_id_map.csv'))\n",
    "    id_dict = {}\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        id_dict[row[-4][3:]] = row[-3]\n",
    "    correct_pmc = set()\n",
    "    for id in pmc:\n",
    "        if id not in id_dict or id_dict[id] == '':\n",
    "            correct_pmc.add('PMC'+id)\n",
    "            continue\n",
    "        correct_pmc.add(id_dict[id])\n",
    "    final_ids = correct_pmc.union(pubmed)\n",
    "    return final_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e272e66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final collection for mortality has 96282 articles\n"
     ]
    }
   ],
   "source": [
    "# TODO: Specify filename\n",
    "article_ids = combine_ids(pmc_ids, pubmed_ids)\n",
    "print(\"Final collection for {} has {} articles\".format(\"mortality\", len(article_ids)))\n",
    "pickle.dump(article_ids, open('../data/outcome-literature/FILENAME', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd7de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split abstracts according to database they are retrieved from\n",
    "# This needs to be done to ensure that we are checking the correct database while retrieving text\n",
    "def split_abstracts(abstracts):\n",
    "    pubmed = []\n",
    "    pmc = []\n",
    "    for abstract in abstracts:\n",
    "        if abstract.startswith('PMC'):\n",
    "            pmc.append(abstract[3:])  # Drop PMC prefix since it is no longer needed to distinguish between PubMed/PMC\n",
    "        else:\n",
    "            pubmed.append(abstract)\n",
    "    return pubmed, pmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ebb620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve complete data for a batch of abstract IDs from a provided database\n",
    "# Results will be retrieved in XML format\n",
    "# TODO: Please add your tool name and email ID in the base_url variable\n",
    "def retrieve_abstract_batch(id_batch, database):\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db={}&id={}&retmode=xml&tool=TOOLNAME&email=EMAIL\"\n",
    "    query = base_url.format(database, ','.join(id_batch))\n",
    "    response = requests.get(query)\n",
    "    xml_abstracts = response.content\n",
    "    return xml_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95df9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse out abstract elements from retrieved XMLs\n",
    "def parse_abstract_xml(xml, database):\n",
    "    top_tag = {'pubmed': 'PubmedArticle', 'pmc': 'article'}  # PubMed and PMC use different XML tags\n",
    "    parsed_xml = ET.fromstring(xml)\n",
    "    if database == 'pmc':\n",
    "        print(ET.tostring(parsed_xml))\n",
    "    articles = parsed_xml.findall(top_tag[database])\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46d03e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure that takes a large set of IDs, breaks it into manageable batches,\n",
    "# queries the provided database and extracts abstracts from retrieved XMLs\n",
    "# TODO: Change file path for storage if needed\n",
    "def retrieve_all_abstracts(id_list, database):\n",
    "    max_query_size = 200  # PubMed only accepts 200 IDs at a time when retrieving abstract text\n",
    "    print('Retrieval will require {} queries'.format(math.ceil(len(id_list)/float(max_query_size))))\n",
    "    retrieved_abstracts = []\n",
    "    texts = {}\n",
    "    for i in range(0, len(id_list), max_query_size):\n",
    "        start = i\n",
    "        end = min(len(id_list), start+max_query_size)\n",
    "        cur_ids = id_list[start:end]\n",
    "        cur_abstracts = retrieve_abstract_batch(cur_ids, database)\n",
    "        cur_parsed_abstracts = parse_abstract_xml(cur_abstracts, database)\n",
    "        if len(cur_parsed_abstracts) != (end-start):\n",
    "            error_log.write('Missing abstracts:\\n')\n",
    "            error_log.write(','.join(cur_ids)+'\\n')\n",
    "        retrieved_abstracts += cur_parsed_abstracts\n",
    "        for abstract in retrieved_abstracts:\n",
    "            pmid = -99999\n",
    "            abstract_text = \"\"\n",
    "            year = -1000\n",
    "            for element in abstract.iter():\n",
    "                if element.tag == 'PMID':\n",
    "                    if pmid == -99999:\n",
    "                        pmid = element.text\n",
    "                if element.tag == 'AbstractText':\n",
    "                    if element.text:\n",
    "                        abstract_text += element.text + '\\n'\n",
    "                if element.tag == 'PubDate':\n",
    "                    for subelement in element.iter():\n",
    "                        if subelement.tag == 'Year':\n",
    "                            year = int(subelement.text )\n",
    "            texts[pmid] = {'text': abstract_text, 'year': year}\n",
    "        if len(texts) % 1000 == 0 or end == len(id_list):\n",
    "            print('Retrieved {} abstracts'.format(end))\n",
    "            retrieved_abstracts = []\n",
    "        pickle.dump(texts, open('../data/{}_texts_and_dates.pkl'.format(database, end), 'wb'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76a16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95988 abstracts will be scraped from PubMed\n",
      "294 abstracts will be scraped from PMC\n",
      "Retrieval will require 480 queries\n",
      "Retrieved 1000 abstracts\n"
     ]
    }
   ],
   "source": [
    "# Running text retrieval for IDs retrieved by outcome-specific queries\n",
    "error_log = open('retrieval_errors.txt', 'w')\n",
    "pubmed_abs, pmc_abs = split_abstracts(article_ids)\n",
    "print('{} abstracts will be scraped from PubMed'.format(len(pubmed_abs)))\n",
    "print('{} abstracts will be scraped from PMC'.format(len(pmc_abs)))\n",
    "retrieve_all_abstracts(pubmed_abs, 'pubmed')\n",
    "error_log.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
