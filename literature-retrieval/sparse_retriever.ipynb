{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40564641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "import scispacy\n",
    "from scispacy.linking import EntityLinker\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bada26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = \"pmv\"  # Switch to los (length of stay) or mortality for other outcomes\n",
    "\n",
    "# Loading MeSH terms for documents from outcome-specific literature collection\n",
    "doc_tags = pickle.load(open('../data/mesh-terms/{}_mesh_terms.pkl'.format(outcome), 'rb'))\n",
    "\n",
    "# Loading MeSH terms for EHRs for patient cohort \n",
    "mention_info = pickle.load(open('../data/mesh-terms/mimic_ehr_mesh_terms.pkl', 'rb'))\n",
    "\n",
    "# Note that the cohort for PMV prediction is smaller than other outcomes\n",
    "# So we need to filter out patients for whom PMV information is not available\n",
    "ids2keep = pickle.load(open('../data/pmv_ids.pkl', 'rb')) if outcome == 'pmv' else None\n",
    "\n",
    "# Reformat EHR MeSH term data\n",
    "ehr_tags = {}\n",
    "for file in mention_info:\n",
    "    if ids2keep is not None and file not in ids2keep:\n",
    "        continue\n",
    "    ehr_mesh_terms = []\n",
    "    for sent in mention_info[file]:\n",
    "        for mention in mention_info[file][sent]:\n",
    "            if 'mesh_ids' not in mention:\n",
    "                continue\n",
    "            for pair in mention['mesh_ids']:\n",
    "                ehr_mesh_terms.append(pair[0])\n",
    "    ehr_tags[file] = ehr_mesh_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b57b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeSH vocabulary size: 8177\n"
     ]
    }
   ],
   "source": [
    "# Compute vocabulary of MeSH terms for TF-IDF vector building\n",
    "mesh_vocab = set([x[0] for y in doc_tags.values() for x in y]).intersection(set([x for y in ehr_tags.values() for x in y]))\n",
    "print('MeSH vocabulary size: {}'.format(len(mesh_vocab)))\n",
    "mesh_vocab = dict(list(zip(list(mesh_vocab), range(len(mesh_vocab)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f805f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct TF-IDF vectors for both outcome-specific literature and EHRs\n",
    "doc_freq = Counter()\n",
    "ehr_vectors = {}\n",
    "article_vectors = {}\n",
    "\n",
    "# Term frequency computation\n",
    "for file in ehr_tags:\n",
    "    term_list = [x for x in ehr_tags[file] if x in mesh_vocab]\n",
    "    doc_freq.update(set(term_list))\n",
    "    cur_vec = [0] * len(mesh_vocab)\n",
    "    for x in term_list:\n",
    "        cur_vec[mesh_vocab[x]] += 1\n",
    "    ehr_vectors[file] = cur_vec\n",
    "for file in doc_tags:\n",
    "    term_list = [x[0] for x in doc_tags[file] if x[0] in mesh_vocab]\n",
    "    doc_freq.update(set(term_list))\n",
    "    cur_vec = [0] * len(mesh_vocab)\n",
    "    for x in term_list:\n",
    "        cur_vec[mesh_vocab[x]] += 1\n",
    "    article_vectors[file] = cur_vec\n",
    "\n",
    "# Incorporating IDF computation\n",
    "num_docs = len(doc_tags) + len(ehr_tags)\n",
    "doc_freq = {k:math.log(num_docs/float(v)) for k,v in doc_freq.items()}\n",
    "doc_freq_vector = [1] * len(mesh_vocab)\n",
    "for x in mesh_vocab:\n",
    "    doc_freq_vector[mesh_vocab[x]] = doc_freq[x]\n",
    "for file in ehr_vectors:\n",
    "    ehr_vectors[file] = [x*y for x,y in zip(ehr_vectors[file], doc_freq_vector)]\n",
    "for file in article_vectors:\n",
    "    article_vectors[file] = [x*y for x,y in zip(article_vectors[file], doc_freq_vector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5ee9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct TF-IDF vector matrices for both literature and outcomes\n",
    "# This helps speed up cosine similarity computation \n",
    "ehr_items = list(ehr_vectors.items())\n",
    "ehr_ids, ehr_matrix = [x[0] for x in ehr_items], [x[1] for x in ehr_items]\n",
    "ehr_matrix = np.vstack(ehr_matrix)\n",
    "article_items = list(article_vectors.items())\n",
    "article_ids, article_matrix = [x[0] for x in article_items], [x[1] for x in article_items]\n",
    "article_matrix = np.vstack(article_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4144a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing cosine similarities and identifying top ranked documents\n",
    "similarities = []\n",
    "ranked_pairs = {}\n",
    "if ehr_matrix.shape[0] < 8000:\n",
    "    similarities = cosine_similarity(ehr_matrix, article_matrix)\n",
    "    top_indices = np.argsort(-1*similarities)[:, :1000]\n",
    "    top_similarities = np.take_along_axis(similarities, top_indices, axis=-1)\n",
    "    top_pairs = np.stack((top_indices, top_similarities), axis=2).tolist()\n",
    "    for i, file in enumerate(ehr_ids):\n",
    "        ranked_pairs[file] = [(article_ids[int(x[0])],x[1]) for x in top_pairs[i]]\n",
    "else:\n",
    "    for start in range(0, ehr_matrix.shape[0], 6000):\n",
    "        print('Computing similarities for slice starting at index {}'.format(start))\n",
    "        end = min(start + 6000, ehr_matrix.shape[0])\n",
    "        cur_ehr_matrix = ehr_matrix[start:end, :]\n",
    "        cur_similarities = cosine_similarity(cur_ehr_matrix, article_matrix)\n",
    "        top_indices = np.argsort(-1*cur_similarities)[:, :1000]\n",
    "        top_similarities = np.take_along_axis(cur_similarities, top_indices, axis=-1)\n",
    "        top_pairs = np.stack((top_indices, top_similarities), axis=2).tolist()\n",
    "        cur_ehr_ids = ehr_ids[start:end]\n",
    "        for i, file in enumerate(cur_ehr_ids):\n",
    "            ranked_pairs[file] = [(article_ids[int(x[0])],x[1]) for x in top_pairs[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dd5f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store ranked results from sparse retriever\n",
    "pickle.dump(ranked_pairs, open('../data/{}_sparse_ranked_docs.pkl'.format(outcome), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
